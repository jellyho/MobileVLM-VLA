{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579a490d-10c9-43a4-9e5a-36dc21d1ac4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA L40S\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Restrict PyTorch to only see GPU 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available, using CPU.\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcfc1628-4a88-43ff-ad9b-4b2b97348037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, get_scheduler\n",
    "from transformers.trainer import ALL_LAYERNORM_LAYERS, get_parameter_names\n",
    "from PIL import Image\n",
    "from accelerate import PartialState\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "from torch.nn import MSELoss, L1Loss, SmoothL1Loss\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from spatialvla.datasets import RLDSDataset, RLDSBatchTransform\n",
    "from spatialvla.datasets.rlds.utils.data_utils import save_dataset_statistics\n",
    "from spatialvla.datasets.rlds.utils.data_utils import PaddedCollatorForActionPrediction\n",
    "\n",
    "from spatialvla.mobilevlm.model.mobilevlm import load_pretrained_vlm_for_vla\n",
    "from spatialvla.mobilevlm.train.train import find_all_linear_names, get_peft_state_maybe_zero_3, get_peft_state_non_lora_maybe_zero_3, find_all_names_from_module\n",
    "\n",
    "from scripts.spatialvla_config import ModelArguments, TrainingArguments, HEAD_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902a3462-d082-478b-8b86-fcfd8fe7033e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jellyho/miniconda3/envs/mobilevlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type mobilevlm to instantiate a model of type spatialvla. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of SpatialVLAForCausalLM were not initialized from the model checkpoint at remyxai/SpaceLLaVA-lite and are newly initialized: ['action_head.diffusion_model.reverse_network.layers.2.dense2.weight', 'action_head.diffusion_model.cond_encoder.mlp.0.bias', 'action_head.diffusion_model.reverse_network.layers.2.dense_residual.bias', 'action_head.diffusion_model.reverse_network.in_dense.bias', 'action_head.diffusion_model.reverse_network.in_dense.weight', 'action_head.diffusion_model.reverse_network.layers.2.dense_residual.weight', 'action_pos', 'action_head.diffusion_model.reverse_network.layers.0.dense_residual.weight', 'action_head.diffusion_model.reverse_network.layers.0.dense1.bias', 'action_head.diffusion_model.reverse_network.layers.0.dense2.bias', 'action_head.diffusion_model.reverse_network.layers.1.layer_norm.bias', 'action_head.diffusion_model.reverse_network.layers.1.dense_residual.bias', 'action_head.diffusion_model.cond_encoder.mlp.1.weight', 'action_head.diffusion_model.reverse_network.layers.2.layer_norm.bias', 'action_head.diffusion_model.reverse_network.out_dense.bias', 'action_head.diffusion_model.cond_encoder.mlp.0.weight', 'action_head.diffusion_model.reverse_network.layers.2.layer_norm.weight', 'action_head.diffusion_model.reverse_network.layers.0.layer_norm.bias', 'action_head.proj.bias', 'action_head.diffusion_model.reverse_network.out_dense.weight', 'action_head.diffusion_model.reverse_network.layers.2.dense2.bias', 'action_head.diffusion_model.reverse_network.layers.1.dense2.weight', 'action_head.diffusion_model.reverse_network.layers.0.dense2.weight', 'action_head.diffusion_model.cond_encoder.mlp.1.bias', 'action_head.diffusion_model.reverse_network.layers.2.dense1.bias', 'action_head.diffusion_model.reverse_network.layers.1.dense2.bias', 'action_head.diffusion_model.reverse_network.layers.1.dense1.bias', 'action_head.diffusion_model.reverse_network.layers.1.dense_residual.weight', 'action_head.proj.weight', 'action_head.diffusion_model.reverse_network.layers.0.dense1.weight', 'action_head.diffusion_model.time_preprocess.kernel', 'action_head.diffusion_model.reverse_network.layers.1.layer_norm.weight', 'action_head.diffusion_model.reverse_network.layers.0.layer_norm.weight', 'action_head.diffusion_model.reverse_network.layers.1.dense1.weight', 'action_head.diffusion_model.reverse_network.layers.2.dense1.weight', 'action_head.diffusion_model.reverse_network.layers.0.dense_residual.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_args = ModelArguments()\n",
    "model_args.action_head = 'Diffusion'\n",
    "model_args.head_args = HEAD_ARGS['Diffusion']\n",
    "training_args = TrainingArguments()\n",
    "dtype = torch.bfloat16\n",
    "device_id = 0\n",
    "tokenizer, model, image_processor, _ = load_pretrained_vlm_for_vla(\n",
    "    model_args, \n",
    "    load_8bit=False, \n",
    "    load_4bit=False,\n",
    "    device=device_id,\n",
    "    dtype=dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e75617b0-46aa-4c3b-b6be-ae8b63032764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from spatialvla.mobilevlm.model.mobilevlm import load_vla, load_pretrained_model\n",
    "from spatialvla.mobilevlm.conversation import conv_templates, SeparatorStyle\n",
    "from spatialvla.mobilevlm.utils import disable_torch_init, process_images, tokenizer_image_token, KeywordsStoppingCriteria\n",
    "from spatialvla.mobilevlm.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, IGNORE_INDEX\n",
    "image = Image.open('lasagna.png')\n",
    "images = [image]\n",
    "images_tensor = process_images(images, image_processor, {'image_aspect_ratio' : 'pad'}).to(model.device, dtype=torch.bfloat16)\n",
    "conv = conv_templates['v1'].copy()\n",
    "conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + 'How many layers it have?')\n",
    "conv.append_message(conv.roles[1], 'hello')\n",
    "prompt = conv.get_prompt()\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "# Input\n",
    "input_ids = (tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda())\n",
    "labels = input_ids.clone()\n",
    "labels[:, :-5] = IGNORE_INDEX\n",
    "attention_mask = input_ids.ne(IGNORE_INDEX)\n",
    "stopping_criteria = KeywordsStoppingCriteria([stop_str],tokenizer, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "373a9bcb-c1f5-4922-b7e8-5ba94aac6725",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = None\n",
    "past_key_values = None\n",
    "inputs_embeds = None\n",
    "# labels = None\n",
    "use_cache = None\n",
    "output_attentions = False\n",
    "output_hidden_states = False\n",
    "return_dict = None\n",
    "actions = None\n",
    "output_attentions = output_attentions if output_attentions is not None else model.config.output_attentions\n",
    "output_hidden_states = (output_hidden_states if output_hidden_states is not None else model.config.output_hidden_states)\n",
    "return_dict = return_dict if return_dict is not None else model.config.use_return_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1155feef-eecf-4e64-b2f6-7170b218a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask, past_key_values, inputs_embeds, labels = \\\n",
    "            model.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39d6f703-e579-411a-836f-27f888e81ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    past_key_values=past_key_values,\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    use_cache=use_cache,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=output_hidden_states,\n",
    "    return_dict=return_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9833b101-f912-46d2-9359-bec747316075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 194, 2048])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e9f67-8881-49c5-9f86-a98b8edc4fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
