{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995ea321-f109-45ba-9ae4-f8877d8bc94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA L40S\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Restrict PyTorch to only see GPU 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available, using CPU.\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2734823b-688b-4d06-adb0-60c51cd14bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 14:05:26,384] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 14:05:26.673581: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-30 14:05:26.702957: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-30 14:05:26.702999: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-30 14:05:26.703763: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-30 14:05:26.708695: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-30 14:05:27.492328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using a model of type mobilevlm to instantiate a model of type spatialvla. This is not supported for all configurations of models and can yield errors.\n",
      "/home/jellyho/miniconda3/envs/mobilevlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "from scripts.inference import VLAModel\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "\n",
    "model_path = \"./SpatialVLA\"\n",
    "model = VLAModel(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ce0087-2695-4de7-9b94-cf14a7e12bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(image, prompt):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default(10)  # You can load a custom font if needed\n",
    "    image_width, image_height = image.size\n",
    "    text_bbox = draw.textbbox((0, 0), prompt, font=font)\n",
    "    text_width = text_bbox[2] - text_bbox[0]  # Width of the text\n",
    "    text_height = text_bbox[3] - text_bbox[1]  # Height of the text\n",
    "    padding = 10  # Padding from the bottom\n",
    "    text_position = ((image_width - text_width) // 2, image_height - text_height - padding)\n",
    "    draw.text(text_position, prompt, font=font, fill=\"white\")  # Text in white color\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0378efea-c3a3-428f-96af-8e1670e25e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                     | 0/226 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[[-0.01106462  0.00346839 -0.00319081  0.01103914  0.05147479 -0.01263141\n",
      "   0.59610183]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "hdf5_files = []\n",
    "folder_path = '/home/shared/LG_Robot/stack_cup/'\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".hdf5\"):\n",
    "        hdf5_files.append(os.path.join(folder_path, file))\n",
    "\n",
    "idx = 20\n",
    "\n",
    "img_list = []\n",
    "data = h5py.File(hdf5_files[idx], 'r')\n",
    "for imgd in tqdm(data['/observation/image']):\n",
    "    img = Image.fromarray(imgd).convert(\"RGB\")\n",
    "    prompt_str = \"What robot should do to transfer the wet tissue to the basket?\"\n",
    "    res = model.inference_action(img, prompt_str)\n",
    "    print(res)\n",
    "    break\n",
    "    # img_list.append(draw(img, res))\n",
    "    # print('action', res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f589feb-8daf-437e-88de-67b544396c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of the first image\n",
    "width, height = img_list[0].size\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\n",
    "video_name = 'output_video.mp4'  # Output video file name\n",
    "video = cv2.VideoWriter(video_name, fourcc, 30, (width, height))  # 30 fps\n",
    "\n",
    "# Step 3: Read each image, convert to array, and write to the video\n",
    "for img in img_list:\n",
    "    # Convert the image to RGB (OpenCV uses BGR format)\n",
    "    img = img.convert(\"RGB\")\n",
    "    \n",
    "    # Convert the Pillow image to a NumPy array\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Convert RGB to BGR (OpenCV's format)\n",
    "    img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Write the image to the video\n",
    "    video.write(img_array)\n",
    "\n",
    "# Step 4: Release the video writer\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Video {video_name} created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d367898-84ab-42d5-9690-a850cd14596f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatialVLAForCausalLM(\n",
       "  (model): SpatialVLAModel(\n",
       "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (vision_tower): CLIPVisionTower(\n",
       "      (vision_tower): CLIPVisionModel(\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "            (position_embedding): Embedding(577, 1024)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mm_projector): LDPNetV2Projector(\n",
       "      (mlp): FeatureIRLayer(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dwn): TokenDownLayer(\n",
       "        (dwn): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=(12, 12))\n",
       "        )\n",
       "      )\n",
       "      (peg): PosInjectLayer(\n",
       "        (peg): Sequential(\n",
       "          (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "  (action_hidden): Linear(in_features=2048, out_features=256, bias=False)\n",
       "  (action_head): Linear(in_features=256, out_features=14, bias=False)\n",
       "  (relu): ReLU()\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adab6d59-793f-4629-a6ff-fc890c08e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.config.save_pretrained(\"./my_model_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce41cf-e7c4-481d-9801-2392875a0f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
