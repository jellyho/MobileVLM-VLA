{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995ea321-f109-45ba-9ae4-f8877d8bc94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA L40S\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Restrict PyTorch to only see GPU 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available, using CPU.\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2734823b-688b-4d06-adb0-60c51cd14bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-31 16:45:01,001] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 16:45:01.312522: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-31 16:45:01.340735: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-31 16:45:01.340779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-31 16:45:01.341660: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-31 16:45:01.346797: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 16:45:02.023600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using a model of type mobilevlm to instantiate a model of type spatialvla. This is not supported for all configurations of models and can yield errors.\n",
      "/home/jellyho/miniconda3/envs/mobilevlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "from scripts.inference import VLAModel\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "\n",
    "model_path = \"./stack_cup_5hz_ln\"\n",
    "model = VLAModel(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ce0087-2695-4de7-9b94-cf14a7e12bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(image, prompt):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default(10)  # You can load a custom font if needed\n",
    "    image_width, image_height = image.size\n",
    "    text_bbox = draw.textbbox((0, 0), prompt, font=font)\n",
    "    text_width = text_bbox[2] - text_bbox[0]  # Width of the text\n",
    "    text_height = text_bbox[3] - text_bbox[1]  # Height of the text\n",
    "    padding = 10  # Padding from the bottom\n",
    "    text_position = ((image_width - text_width) // 2, image_height - text_height - padding)\n",
    "    draw.text(text_position, prompt, font=font, fill=\"white\")  # Text in white color\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0378efea-c3a3-428f-96af-8e1670e25e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                     | 0/226 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[[-0.01106462  0.00346839 -0.00319081  0.01103914  0.05147479 -0.01263141\n",
      "   0.59610183]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "hdf5_files = []\n",
    "folder_path = '/home/shared/LG_Robot/stack_cup/'\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".hdf5\"):\n",
    "        hdf5_files.append(os.path.join(folder_path, file))\n",
    "\n",
    "idx = 20\n",
    "\n",
    "img_list = []\n",
    "data = h5py.File(hdf5_files[idx], 'r')\n",
    "for imgd in tqdm(data['/observation/image']):\n",
    "    img = Image.fromarray(imgd).convert(\"RGB\")\n",
    "    prompt_str = \"What robot should do to transfer the wet tissue to the basket?\"\n",
    "    res = model.inference_action(img, prompt_str)\n",
    "    print(res)\n",
    "    break\n",
    "    # img_list.append(draw(img, res))\n",
    "    # print('action', res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f589feb-8daf-437e-88de-67b544396c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of the first image\n",
    "width, height = img_list[0].size\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\n",
    "video_name = 'output_video.mp4'  # Output video file name\n",
    "video = cv2.VideoWriter(video_name, fourcc, 30, (width, height))  # 30 fps\n",
    "\n",
    "# Step 3: Read each image, convert to array, and write to the video\n",
    "for img in img_list:\n",
    "    # Convert the image to RGB (OpenCV uses BGR format)\n",
    "    img = img.convert(\"RGB\")\n",
    "    \n",
    "    # Convert the Pillow image to a NumPy array\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Convert RGB to BGR (OpenCV's format)\n",
    "    img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Write the image to the video\n",
    "    video.write(img_array)\n",
    "\n",
    "# Step 4: Release the video writer\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Video {video_name} created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adab6d59-793f-4629-a6ff-fc890c08e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.config.save_pretrained(\"./my_model_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fce41cf-e7c4-481d-9801-2392875a0f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 336, 336])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "ðŸš€ SpaceLLaVA-lite: Teach history\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scripts.inference import inference_once\n",
    "model_path = \"remyxai/SpaceLLaVA-lite\"\n",
    "image_file = \"assets/samples/demo.jpg\"\n",
    "prompt_str = \"What is the purpose of this book?\\nAnswer the question using a single word or phrase.\"\n",
    "\n",
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"image_file\": image_file,\n",
    "    \"prompt\": prompt_str,\n",
    "    \"conv_mode\": \"v1\",\n",
    "    \"temperature\": 0, \n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"load_8bit\": False,\n",
    "    \"load_4bit\": False,\n",
    "})()\n",
    "\n",
    "inference_once(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18eb8e64-5c04-456d-a54b-740d6395c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.trainer import ALL_LAYERNORM_LAYERS\n",
    "from PIL import Image\n",
    "from accelerate import PartialState\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "from torch.nn import MSELoss, L1Loss, SmoothL1Loss\n",
    "from torch.optim import AdamW, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from dataset.dataset import RLDSDataset, save_statistics_to_json\n",
    "from mobilevlm.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from mobilevlm import conversation as conversation_lib\n",
    "from mobilevlm.utils import tokenizer_image_token\n",
    "from mobilevlm.model.mobilevlm import load_pretrained_vlm_for_vla\n",
    "from mobilevlm.model.mobilellama import MobileLlamaForCausalLM, SpatialVLAForCausalLM, MobileVLMConfig, SpatialVLAConfig\n",
    "from mobilevlm.train.train import find_all_linear_names, get_peft_state_maybe_zero_3, get_peft_state_non_lora_maybe_zero_3\n",
    "from scripts.mergelora import merge_lora\n",
    "\n",
    "from scripts.spatialvla_config import dataset_kwargs, traj_transform_kwargs, frame_transform_kwargs, ModelArguments, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc6e2d97-e673-4b28-950a-8d334ea6e46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jellyho/miniconda3/envs/mobilevlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of SpatialVLAForCausalLM were not initialized from the model checkpoint at remyxai/SpaceLLaVA-lite and are newly initialized: ['action_head.model.2.weight', 'action_head.model.0.bias', 'action_head.model.2.bias', 'action_head.model.0.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "model_args = ModelArguments()\n",
    "tokenizer, model, image_processor, _ = load_pretrained_vlm_for_vla(\n",
    "    model_args.model_path, \n",
    "    False, \n",
    "    False,\n",
    "    device='cuda',\n",
    "    action_len=model_args.action_len,\n",
    "    action_dim=model_args.action_dim,\n",
    "    action_hidden_sizes=model_args.action_hidden_sizes,\n",
    "    hidden_projection=model_args.hidden_projection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fd11579-6fe1-419d-b1c9-bc66ab61b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained VLM Loaded\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "model = model.cuda()\n",
    "for p in model.get_model().mm_projector.parameters():\n",
    "    p.requires_grad = True\n",
    "for p in model.action_head.parameters():\n",
    "    p.requires_grad = True\n",
    "print('Pretrained VLM Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43c6d101-2736-4536-acdc-c59d442e6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "training_args = TrainingArguments()\n",
    "lora_config = LoraConfig(\n",
    "    r=training_args.lora_r,\n",
    "    lora_alpha=training_args.lora_alpha,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    lora_dropout=training_args.lora_dropout,\n",
    "    bias=training_args.lora_bias,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "if training_args.bits == 16:\n",
    "    if training_args.bf16:\n",
    "        model.to(torch.bfloat16)\n",
    "    if training_args.fp16:\n",
    "        model.to(torch.float16)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a73ba558-6ae9-4f23-93db-b903bbaa994b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 76,836,622 || all params: 1,744,641,806 || trainable%: 4.4041\n"
     ]
    }
   ],
   "source": [
    "for p in model.get_model().mm_projector.parameters():\n",
    "    p.requires_grad = True\n",
    "for p in model.action_head.parameters():\n",
    "    p.requires_grad = True\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8adce47b-57c6-408c-af63-abc8d1cedbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer import ALL_LAYERNORM_LAYERS, get_parameter_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "150f4fc5-2e43-4d7c-b117-08bbac8538bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_parameters = get_parameter_names(model.model, ALL_LAYERNORM_LAYERS)\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "unused_parameters = [name for name, _ in model.model.named_parameters() if \"vision_tower\" in name and \"layers\" not in name]\n",
    "projector_parameters = [name for name, _ in model.model.named_parameters() if \"mm_projector\" in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30e90e54-113c-4bb1-af82-439ca0d3a866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['action_head.model.0.bias', 'action_head.model.2.bias']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n, p in model.model.named_parameters() if (n not in decay_parameters and n not in projector_parameters and n not in unused_parameters and p.requires_grad)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc2962eb-922c-4e84-b3cd-ea8cbf840679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.0.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.0.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.0.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.0.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.0.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.0.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.0.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.0.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.0.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.0.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.0.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.0.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.0.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.0.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.1.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.1.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.1.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.1.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.1.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.1.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.1.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.1.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.1.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.1.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.1.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.1.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.1.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.1.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.2.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.2.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.2.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.2.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.2.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.2.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.2.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.2.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.2.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.2.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.2.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.2.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.2.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.2.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.3.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.3.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.3.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.3.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.3.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.3.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.3.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.3.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.3.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.3.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.3.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.3.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.3.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.3.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.4.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.4.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.4.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.4.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.4.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.4.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.4.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.4.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.4.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.4.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.4.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.4.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.4.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.4.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.5.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.5.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.5.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.5.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.5.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.5.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.5.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.5.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.5.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.5.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.5.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.5.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.5.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.5.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.6.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.6.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.6.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.6.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.6.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.6.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.6.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.6.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.6.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.6.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.6.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.6.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.6.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.6.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.7.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.7.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.7.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.7.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.7.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.7.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.7.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.7.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.7.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.7.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.7.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.7.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.7.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.7.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.8.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.8.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.8.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.8.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.8.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.8.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.8.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.8.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.8.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.8.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.8.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.8.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.8.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.8.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.9.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.9.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.9.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.9.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.9.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.9.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.9.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.9.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.9.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.9.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.9.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.9.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.9.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.9.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.10.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.10.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.10.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.10.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.10.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.10.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.10.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.10.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.10.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.10.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.10.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.10.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.10.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.10.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.11.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.11.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.11.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.11.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.11.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.11.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.11.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.11.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.11.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.11.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.11.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.11.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.11.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.11.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.12.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.12.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.12.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.12.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.12.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.12.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.12.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.12.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.12.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.12.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.12.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.12.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.12.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.12.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.13.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.13.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.13.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.13.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.13.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.13.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.13.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.13.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.13.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.13.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.13.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.13.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.13.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.13.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.14.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.14.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.14.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.14.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.14.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.14.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.14.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.14.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.14.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.14.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.14.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.14.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.14.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.14.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.15.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.15.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.15.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.15.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.15.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.15.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.15.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.15.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.15.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.15.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.15.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.15.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.15.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.15.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.16.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.16.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.16.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.16.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.16.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.16.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.16.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.16.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.16.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.16.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.16.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.16.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.16.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.16.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.17.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.17.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.17.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.17.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.17.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.17.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.17.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.17.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.17.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.17.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.17.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.17.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.17.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.17.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.18.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.18.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.18.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.18.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.18.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.18.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.18.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.18.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.18.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.18.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.18.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.18.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.18.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.18.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.19.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.19.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.19.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.19.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.19.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.19.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.19.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.19.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.19.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.19.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.19.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.19.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.19.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.19.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.20.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.20.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.20.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.20.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.20.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.20.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.20.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.20.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.20.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.20.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.20.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.20.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.20.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.20.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.21.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.21.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.21.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.21.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.21.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.21.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.21.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.21.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.21.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.21.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.21.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.21.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.21.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.21.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.22.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.22.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.22.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.22.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.22.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.22.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.22.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.22.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.22.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.22.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.22.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.22.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.22.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.22.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.layers.23.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.layers.23.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.layers.23.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.layers.23.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.layers.23.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.layers.23.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.layers.23.self_attn.o_proj.lora_A.default.weight',\n",
       " 'model.layers.23.self_attn.o_proj.lora_B.default.weight',\n",
       " 'model.layers.23.mlp.gate_proj.lora_A.default.weight',\n",
       " 'model.layers.23.mlp.gate_proj.lora_B.default.weight',\n",
       " 'model.layers.23.mlp.up_proj.lora_A.default.weight',\n",
       " 'model.layers.23.mlp.up_proj.lora_B.default.weight',\n",
       " 'model.layers.23.mlp.down_proj.lora_A.default.weight',\n",
       " 'model.layers.23.mlp.down_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_B.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_A.default.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_B.default.weight',\n",
       " 'action_head.model.0.weight',\n",
       " 'action_head.model.2.weight']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n, p in model.model.named_parameters() if (n in decay_parameters and n not in projector_parameters and n not in unused_parameters and p.requires_grad)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3852d3b-394f-4153-a4b0-cfd5ee835e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.mm_projector.mlp.mlp.0.weight',\n",
       " 'model.mm_projector.mlp.mlp.2.weight',\n",
       " 'model.mm_projector.peg.peg.0.weight']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n, p in model.model.named_parameters() if (n in decay_parameters and n in projector_parameters and n not in unused_parameters and p.requires_grad)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf047a72-e91a-4946-bef2-e4449f36a05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.mm_projector.mlp.mlp.0.bias',\n",
       " 'model.mm_projector.mlp.mlp.2.bias',\n",
       " 'model.mm_projector.peg.peg.0.bias']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n, p in model.model.named_parameters() if (n not in decay_parameters and n in projector_parameters and n not in unused_parameters and p.requires_grad)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65815d6c-ebc6-4a1b-a5ad-f8aeaa467a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding',\n",
       " 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias',\n",
       " 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight',\n",
       " 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unused_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40cb0157-3b4f-4f7b-b0cd-5b0dade76e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.nn.modules.normalization.LayerNorm]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_LAYERNORM_LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891049a6-4ddd-4566-817a-67254305d50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
