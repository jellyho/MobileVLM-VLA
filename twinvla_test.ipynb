{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff70a88e-cd9b-40c4-a5cb-c0d829b2a1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-07 17:35:21,265] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 17:35:22.826945: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-07 17:35:22.856175: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-07 17:35:22.856218: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-07 17:35:22.857125: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-07 17:35:22.862990: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-07 17:35:25.386608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from spatialvla.mobilevlm.model.bimanual import load_twinvla_from_singlevla, load_twinvla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d89004-5815-41d5-8d44-ac01a97fde0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type spatialvla to instantiate a model of type twinvla. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jellyho/miniconda3/envs/mobilevlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model, image_processor, _ = load_twinvla_from_singlevla('checkpoints/libero_object_octo_full_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a8cc4-254f-40b6-82d1-f5c715eec2f6",
   "metadata": {},
   "source": [
    "### Trainig Code Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf883613-2345-4f03-ba70-c65f5784f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spatialvla.datasets import RLDSBatchTransform, RLDSDataset\n",
    "from spatialvla.mobilevlm.model.mobilevlm import load_pretrained_vlm_for_vla, load_vla\n",
    "from scripts.spatialvla_config import ModelArguments, TrainingArguments\n",
    "import transformers\n",
    "from spatialvla.datasets.rlds.utils.data_utils import PaddedCollatorForActionPrediction\n",
    "from torch.utils.data import DataLoader\n",
    "from spatialvla.mobilevlm.action_tokenizer import ActionTokenizer\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from spatialvla.mobilevlm.model.mobilevlm import load_vla, load_pretrained_model\n",
    "from spatialvla.mobilevlm.conversation import conv_templates, SeparatorStyle\n",
    "from spatialvla.mobilevlm.utils import disable_torch_init, process_images, tokenizer_image_token, KeywordsStoppingCriteria\n",
    "from spatialvla.mobilevlm.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Tuple, Type\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from spatialvla.mobilevlm.utils import disable_torch_init, process_images, tokenizer_image_token, KeywordsStoppingCriteria\n",
    "# from prismatic.models.backbones.llm.prompting import PromptBuilder\n",
    "# from prismatic.models.backbones.vision import ImageTransform\n",
    "from spatialvla.mobilevlm.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from spatialvla.mobilevlm.conversation import conv_templates, SeparatorStyle\n",
    "from spatialvla.datasets.rlds.utils.data_utils import tree_map\n",
    "# from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from spatialvla.datasets.rlds import make_interleaved_dataset, make_single_dataset\n",
    "from spatialvla.datasets.rlds.oxe import OXE_NAMED_MIXTURES, get_oxe_dataset_kwargs_and_weights\n",
    "from spatialvla.datasets.rlds.utils.data_utils import NormalizationType\n",
    "from transformers import PreTrainedTokenizerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf98317-f7ef-4fe3-ab3a-16237826ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_transform = RLDSBatchTransform(\n",
    "    tokenizer,\n",
    "    image_processor,\n",
    "    use_state_input=True,\n",
    "    window_size=1,\n",
    "    future_action_window_size=7,\n",
    ")\n",
    "vla_dataset = RLDSDataset(\n",
    "    data_root_dir='/data1/OXE',\n",
    "    data_mix='fractal20220817_data',\n",
    "    batch_transform=batch_transform,\n",
    "    shuffle_buffer_size=100,\n",
    "    window_size=1,\n",
    "    future_action_window_size=7,\n",
    "    train=True,\n",
    "    use_state_input=True\n",
    ")\n",
    "collator = PaddedCollatorForActionPrediction(\n",
    "    tokenizer.model_max_length, \n",
    "    tokenizer.pad_token_id, \n",
    "    padding_side='right', \n",
    "    use_state_input=True,\n",
    "    use_label=False\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    vla_dataset,\n",
    "    batch_size=8,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f16bbc-463d-4b6f-a187-e38afaa2dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.training=False\n",
    "model.config.save_pretrained('checkpoints/twinvla')\n",
    "model.save_pretrained('checkpoints/twinvla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de07103b-3d78-4f81-8115-ba22123678f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('checkpoints/twinvla/tokenizer_config.json',\n",
       " 'checkpoints/twinvla/special_tokens_map.json',\n",
       " 'checkpoints/twinvla/tokenizer.model',\n",
       " 'checkpoints/twinvla/added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('checkpoints/twinvla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e809951c-17e7-48c8-bee9-4ac2838d8517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jellyho/miniconda3/envs/mobilevlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_t, twinvla, image_processor_t, dataset_statistics_t = load_twinvla('checkpoints/twinvla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b36465d-14f2-4590-a9a0-26055ade5f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwinVLA(\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "  (action_head): DiffusionActionHead(\n",
       "    (diffusion_model): ScoreActor(\n",
       "      (time_preprocess): FourierFeatures()\n",
       "      (cond_encoder): MLP(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (reverse_network): MLPResNet(\n",
       "        (in_dense): Linear(in_features=295, out_features=256, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x MLPResNetBlock(\n",
       "            (activation): SiLU()\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dense1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dense_residual): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (out_dense): Linear(in_features=256, out_features=7, bias=True)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  )\n",
       "  (model_r): SpatialVLAModel(\n",
       "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (mm_projector): LDPNetV2Projector(\n",
       "      (mlp): FeatureIRLayer(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dwn): TokenDownLayer(\n",
       "        (dwn): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=(12, 12))\n",
       "        )\n",
       "      )\n",
       "      (peg): PosInjectLayer(\n",
       "        (peg): Sequential(\n",
       "          (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model_l): SpatialVLAModel(\n",
       "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (mm_projector): LDPNetV2Projector(\n",
       "      (mlp): FeatureIRLayer(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dwn): TokenDownLayer(\n",
       "        (dwn): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=(12, 12))\n",
       "        )\n",
       "      )\n",
       "      (peg): PosInjectLayer(\n",
       "        (peg): Sequential(\n",
       "          (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vision_tower): CLIPVisionTower(\n",
       "    (vision_tower): CLIPVisionModel(\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "          (position_embedding): Embedding(577, 1024)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twinvla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936afe8b-ecb9-4f2d-8caf-4caf9bd8b392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.distributions.beta.Beta"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.distributions.Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aff44720-4afd-477c-b31a-501fa1bdf2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jellyho/Bimanual_Imitation/MobileVLM-VLA/spatialvla/datasets/datasets.py:80: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  proprio = torch.Tensor(rlds_batch['observation']['proprio']) if self.use_state_input else None\n"
     ]
    }
   ],
   "source": [
    "for d in dataloader:\n",
    "    batch = d\n",
    "    break\n",
    "device_id = 0\n",
    "input_ids=batch['input_ids'].to(device_id)\n",
    "images=batch['pixel_values'].to(device_id)\n",
    "attention_mask=batch['attention_mask'].to(device_id)\n",
    "actions=batch['action'].to(device_id)\n",
    "use_cache=True\n",
    "# states=batch['proprio'].to(device_id)\n",
    "past_key_values = None\n",
    "# labels = batch['labels'].to(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e23967d7-7133-462b-9b30-1238818cc310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 336, 336])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 336), <f2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mobilevlm/lib/python3.10/site-packages/PIL/Image.py:3277\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3277\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   3278\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 336), '<f2')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(images[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m i\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmapel.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m img \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/miniconda3/envs/mobilevlm/lib/python3.10/site-packages/PIL/Image.py:3281\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3279\u001b[0m         typekey_shape, typestr \u001b[38;5;241m=\u001b[39m typekey\n\u001b[1;32m   3280\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypekey_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypestr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   3282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3283\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 336), <f2"
     ]
    }
   ],
   "source": [
    "import mediapy\n",
    "# print(images[0][0].cpu().transpose(0, 2))\n",
    "from PIL import Image\n",
    "print(images[0][0].shape)\n",
    "i = Image.fromarray(images[0][0].cpu().numpy())\n",
    "i.save('smapel.png')\n",
    "img = images[0][0].cpu()\n",
    "mediapy.show_image(img.transpose(1, 2).transpose(0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ecac68-2a5a-47c0-b66e-163c2632e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Memory Allocated: 5739.42 MB\n",
      "Reserved Memory: 6502.00 MB\n",
      "Peak Memory Allocated: 6318.32 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current GPU memory usage\n",
    "    current_memory = torch.cuda.memory_allocated()  # Memory currently allocated by tensors\n",
    "    reserved_memory = torch.cuda.memory_reserved()  # Total memory reserved by PyTorch\n",
    "    max_memory = torch.cuda.max_memory_allocated()  # Peak memory usage for allocated tensors\n",
    "\n",
    "    # Convert to MB for readability\n",
    "    current_memory_mb = current_memory / (1024 ** 2)\n",
    "    reserved_memory_mb = reserved_memory / (1024 ** 2)\n",
    "    max_memory_mb = max_memory / (1024 ** 2)\n",
    "\n",
    "    print(f\"Current Memory Allocated: {current_memory_mb:.2f} MB\")\n",
    "    print(f\"Reserved Memory: {reserved_memory_mb:.2f} MB\")\n",
    "    print(f\"Peak Memory Allocated: {max_memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7f1de84-0069-48df-9f51-4e33f84cee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "    action  = model.predict_action(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        images=images,\n",
    "        # actions=actions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a67b8cb8-27e3-40f0-a793-436dd0c76c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0334, -1.6053,  0.1652, -0.0785,  1.7312,  0.2530,  0.9703],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(action[0][0][7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "139e9e42-d6d1-4796-b86e-04069012d33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1909, -1.6040,  0.1831, -0.0996,  1.5888,  0.5236,  1.0293],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(action[0][0][:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "750a5770-2964-497f-925e-eb3308da206d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1050, -1.6885,  0.1926, -0.0505,  1.4863,  0.3569,  1.0000],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[0][00]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
