{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842a2262-3445-4f95-b729-9fa4eda679a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 20:55:49.069911: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-13 20:55:49.101521: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-13 20:55:49.101572: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-13 20:55:49.102800: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-13 20:55:49.108874: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-13 20:55:51.670860: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 20:56:01,964] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from spatialvla.datasets import RLDSBatchTransform, RLDSDataset\n",
    "from spatialvla.mobilevlm.model.mobilevlm import load_pretrained_vlm_for_vla, load_vla\n",
    "from scripts.spatialvla_config import ModelArguments, TrainingArguments\n",
    "import transformers\n",
    "from spatialvla.datasets.rlds.utils.data_utils import PaddedCollatorForActionPrediction\n",
    "from torch.utils.data import DataLoader\n",
    "from spatialvla.mobilevlm.action_tokenizer import ActionTokenizer\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from spatialvla.mobilevlm.model.mobilevlm import load_vla, load_pretrained_model\n",
    "from spatialvla.mobilevlm.conversation import conv_templates, SeparatorStyle\n",
    "from spatialvla.mobilevlm.utils import disable_torch_init, process_images, tokenizer_image_token, KeywordsStoppingCriteria\n",
    "from spatialvla.mobilevlm.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Tuple, Type\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from spatialvla.mobilevlm.utils import disable_torch_init, process_images, tokenizer_image_token, KeywordsStoppingCriteria\n",
    "# from prismatic.models.backbones.llm.prompting import PromptBuilder\n",
    "# from prismatic.models.backbones.vision import ImageTransform\n",
    "\n",
    "from spatialvla.mobilevlm.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from spatialvla.mobilevlm.conversation import conv_templates, SeparatorStyle\n",
    "from spatialvla.mobilevlm.model.bimanual import load_twinvla\n",
    "from spatialvla.datasets.rlds.utils.data_utils import tree_map\n",
    "# from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from spatialvla.datasets.rlds import make_interleaved_dataset, make_single_dataset\n",
    "from spatialvla.datasets.rlds.oxe import OXE_NAMED_MIXTURES, get_oxe_dataset_kwargs_and_weights\n",
    "from spatialvla.datasets.rlds.utils.data_utils import NormalizationType\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f7dda9-61ac-4134-b12d-06918c27976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jellyho/miniconda3/envs/mobilevlm/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/jellyho/miniconda3/envs/mobilevlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model, image_processor, _ = load_vla('checkpoints/libero_object_fm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675a5c40-9f66-4f6a-8068-fea7dc017c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'libero_object_no_noops', 'data_dir': '/home/shared/rlds_datasets', 'image_obs_keys': {'primary': 'image', 'secondary': None}, 'absolute_action_mask': [False, False, False, False, False, False, False], 'action_normalization_mask': [True, True, True, True, True, True, True], 'action_proprio_normalization_type': <NormalizationType.BOUNDS_Q99: 'bounds_q99'>, 'language_key': 'language_instruction', 'standardize_fn': <function libero_dataset_transform at 0x14f2f9835750>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 20:56:26.513447: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2025-02-13 20:56:27.226363: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################################\n",
      "# Loading the following 1 datasets (incl. sampling weight):                         #\n",
      "# libero_object_no_noops: ==================================================1.000000 #\n",
      "######################################################################################\n",
      "\n",
      "Threads per Dataset:  [1]\n",
      "Reads per Dataset:  [1]\n",
      "Constructing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 20:56:27.646783: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying frame transforms on dataset...\n"
     ]
    }
   ],
   "source": [
    "from spatialvla.mobilevlm.action_tokenizer import FASTTokenizer\n",
    "\n",
    "fast_tokenizer = FASTTokenizer(tokenizer)\n",
    "\n",
    "batch_transform = RLDSBatchTransform(\n",
    "    tokenizer,\n",
    "    image_processor,\n",
    "    use_state_input=False,\n",
    "    window_size=1,\n",
    "    future_action_window_size=31,\n",
    "    use_hz_input=True,\n",
    "    action_tokenizer=fast_tokenizer\n",
    ")\n",
    "vla_dataset = RLDSDataset(\n",
    "    data_root_dir='/home/shared/rlds_datasets',\n",
    "    data_mix='libero_object_no_noops',\n",
    "    batch_transform=batch_transform,\n",
    "    shuffle_buffer_size=10000,\n",
    "    window_size=1,\n",
    "    future_action_window_size=31,\n",
    "    use_state_input = False,\n",
    "    quantile_norm = True\n",
    ")\n",
    "collator = PaddedCollatorForActionPrediction(\n",
    "    tokenizer.model_max_length, \n",
    "    tokenizer.pad_token_id, \n",
    "    padding_side='right', \n",
    "    use_state_input=False,\n",
    "    use_label=True,\n",
    "    use_hz_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7ebda6-ae8b-40e3-8bc0-d7c7f9f73cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    vla_dataset,\n",
    "    batch_size=4,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35cfbb4e-7854-4e42-aa5b-5d9ed5c4089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = None\n",
    "for b in dataloader:\n",
    "    batch = b\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47a68135-e6c8-4761-a38d-727d5ab69bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
       "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
       "         29889,  3148,  1001, 29901, 29871,  -200, 29871,    13,  5618,  3158,\n",
       "           881,   278, 19964,  2125,   304,  5839,   701,   278, 24841,  3623,\n",
       "           625,   322,  2058,   372,   297,   278, 25972, 29973,   319,  1799,\n",
       "          9047, 13566, 29901, 31716, 31688, 31702, 31697, 30264, 31596, 31728,\n",
       "         31381, 31699, 31742, 31121, 31598, 31588, 31716, 31688, 31710, 31602,\n",
       "         30991, 31445, 31703, 31716, 30936, 31600, 31322, 31701, 31713, 31320,\n",
       "         31713, 31742, 31523, 31644, 31710, 31639, 31706, 31222, 31666, 30353,\n",
       "         31604, 31416, 31604,     0],\n",
       "        [    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
       "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
       "         29889,  3148,  1001, 29901, 29871,  -200, 29871,    13,  5618,  3158,\n",
       "           881,   278, 19964,  2125,   304,  5839,   701,   278,  6454,  1219,\n",
       "         12507,   346,   322,  2058,   372,   297,   278, 25972, 29973,   319,\n",
       "          1799,  9047, 13566, 29901, 31615, 31716, 31615, 30987, 31630, 31642,\n",
       "         31206, 31673, 31624, 31608, 31539, 31705, 31645, 31716, 31640, 31726,\n",
       "         31486, 30663, 31246, 31703, 31645, 30921, 31698, 31357, 31342, 31562,\n",
       "         31519, 31706, 31644, 31706, 31741, 31320, 31741, 31317, 31586, 31068,\n",
       "         31741, 31317, 29977,     0],\n",
       "        [    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
       "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
       "         29889,  3148,  1001, 29901, 29871,  -200, 29871,    13,  5618,  3158,\n",
       "           881,   278, 19964,  2125,   304,  5839,   701,   278, 22968, 22300,\n",
       "           322,  2058,   372,   297,   278, 25972, 29973,   319,  1799,  9047,\n",
       "         13566, 29901, 31600, 31580, 31642, 31565, 31719, 30991, 31640, 31717,\n",
       "         31651, 31708, 31551, 31733, 31707, 31397, 31444, 31698, 31568, 31274,\n",
       "         31734, 31673, 31517, 31722, 31541, 30861, 31704, 31037, 31717, 31654,\n",
       "         31137, 31741, 31317, 31712, 31096, 31741, 31320, 31713, 31646, 31741,\n",
       "         31317, 30754, 31644, 31643],\n",
       "        [    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
       "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
       "         29889,  3148,  1001, 29901, 29871,  -200, 29871,    13,  5618,  3158,\n",
       "           881,   278, 19964,  2125,   304,  5839,   701,   278, 24841,  3623,\n",
       "           625,   322,  2058,   372,   297,   278, 25972, 29973,   319,  1799,\n",
       "          9047, 13566, 29901, 31678, 31605, 31704, 31595, 31685, 31535, 31477,\n",
       "         30880, 30913, 30265, 31330, 31410, 31441, 31306, 31698, 31417, 31666,\n",
       "         31644, 30288, 31066, 31738, 31711,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "895fddda-746e-4338-8461-74ae1a686690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100, 31716, 31688, 31702, 31697, 30264, 31596, 31728,\n",
       "         31381, 31699, 31742, 31121, 31598, 31588, 31716, 31688, 31710, 31602,\n",
       "         30991, 31445, 31703, 31716, 30936, 31600, 31322, 31701, 31713, 31320,\n",
       "         31713, 31742, 31523, 31644, 31710, 31639, 31706, 31222, 31666, 30353,\n",
       "         31604, 31416, 31604,  -100],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100, 31615, 31716, 31615, 30987, 31630, 31642,\n",
       "         31206, 31673, 31624, 31608, 31539, 31705, 31645, 31716, 31640, 31726,\n",
       "         31486, 30663, 31246, 31703, 31645, 30921, 31698, 31357, 31342, 31562,\n",
       "         31519, 31706, 31644, 31706, 31741, 31320, 31741, 31317, 31586, 31068,\n",
       "         31741, 31317, 29977,  -100],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100, 31600, 31580, 31642, 31565, 31719, 30991, 31640, 31717,\n",
       "         31651, 31708, 31551, 31733, 31707, 31397, 31444, 31698, 31568, 31274,\n",
       "         31734, 31673, 31517, 31722, 31541, 30861, 31704, 31037, 31717, 31654,\n",
       "         31137, 31741, 31317, 31712, 31096, 31741, 31320, 31713, 31646, 31741,\n",
       "         31317, 30754, 31644, 31643],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100, 31678, 31605, 31704, 31595, 31685, 31535, 31477,\n",
       "         30880, 30913, 30265, 31330, 31410, 31441, 31306, 31698, 31417, 31666,\n",
       "         31644, 30288, 31066, 31738, 31711,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0ea3d-33e8-43ce-8da9-928942c726b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast = AutoProcessor.from_pretrained(\"physical-intelligence/fast\", trust_remote_code=True)\n",
    "batch = None\n",
    "tokenized = []\n",
    "original = []\n",
    "list_mse = []\n",
    "count = 0\n",
    "for b in dataloader:\n",
    "    batch = b\n",
    "    action = batch['action']\n",
    "    tokens = fast(action)\n",
    "    decoded_actions = fast.decode(tokens, time_horizon=10, action_dim=7)\n",
    "    mse = ((action - decoded_actions) ** 2).mean().cpu().numpy()\n",
    "    for t in tokens:\n",
    "        tokenized.append(len(t))\n",
    "        original.append(10 * 7)\n",
    "    list_mse.append(mse)\n",
    "    count += 1\n",
    "    print(count)\n",
    "    if count == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a001a-de72-42a7-86d2-a03dac225749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d68b4282-e146-4a35-a742-4b091c91ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_rate = np.array(tokenized).sum() / np.array(original).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "935ec7ad-d086-48a4-80a5-6dc26b666e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24231696428571428"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1bda6b1-c925-4479-b4c8-08c61de30c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00048670708040417785"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list_mse).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba6d7f36-0882-4ccc-924c-fbdc581aafc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = batch['action']\n",
    "padded_array = np.pad(action, ((0, 0), (0, 0), (0, 32 - action.shape[-1])), mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84c9378c-586e-4090-aba3-ab82bd1ff9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 7])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e953b627-6f81-47e8-802e-12f3add9c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "# Load the tokenizer from the Hugging Face hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20df0ae5-f455-4110-b6d3-908f36d64b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c524a618-febd-418a-968b-52974f3a12c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c00e3e33-e5ad-4988-8434-8cc791628cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize & decode action chunks (we use dummy data here)\n",
    "tokens = fast(action)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e415249-7820-4ffa-9063-4f48902dd64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b06d896b-a2af-444b-958d-0f1bdaa7d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_actions = fast.decode(tokens, time_horizon=10, action_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68fb8ce8-740a-4f98-9d7f-1d8e0b09c6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005500334909725041"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((decoded_actions[0][:, :7] - action[0].cpu().numpy())**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7681128e-f97a-4b7c-9db2-0cc4020718c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (16,15,7) (16,15,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ((\u001b[43mdecoded_actions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (16,15,7) (16,15,6) "
     ]
    }
   ],
   "source": [
    "((decoded_actions - batch['action'][:, :, :-1].cpu().numpy()) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a7696b32-25be-44a2-b210-b21291ab24db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0832,  0.3613,  0.4915, -0.4893,  0.4194,  0.0609,  1.0000],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['action'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd87043b-dfca-4255-9e6d-9dd2d4cd68a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0322c224-cf21-4b72-8ff5-c26c6ca169a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
