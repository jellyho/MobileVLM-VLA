{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca031cb6-7b2e-445c-a4ea-933d43947123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 14:26:44.974831: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-24 14:26:45.002832: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-24 14:26:45.002882: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-24 14:26:45.003557: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-24 14:26:45.008880: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-24 14:26:45.628010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-24 14:26:48,530] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, get_scheduler\n",
    "from transformers.trainer import ALL_LAYERNORM_LAYERS, get_parameter_names\n",
    "from PIL import Image\n",
    "from accelerate import PartialState\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "from torch.nn import MSELoss, L1Loss, SmoothL1Loss\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from spatialvla.datasets import RLDSDataset, RLDSBatchTransform\n",
    "from spatialvla.datasets.rlds.utils.data_utils import save_dataset_statistics\n",
    "from spatialvla.datasets.rlds.utils.data_utils import PaddedCollatorForActionPrediction\n",
    "\n",
    "from spatialvla.mobilevlm.model.mobilevlm import load_pretrained_vlm_for_vla\n",
    "from spatialvla.mobilevlm.train.train import find_all_linear_names, get_peft_state_maybe_zero_3, get_peft_state_non_lora_maybe_zero_3, find_all_names_from_module\n",
    "\n",
    "from scripts.spatialvla_config import ModelArguments, TrainingArguments, HEAD_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "472a03c6-cf96-4a3a-9485-6638cff8471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments()\n",
    "model_args.action_head = 'DiT'\n",
    "model_args.head_args = HEAD_ARGS['DiT']\n",
    "training_args = TrainingArguments()\n",
    "dtype = torch.bfloat16\n",
    "device_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa55c97-d0b1-47c4-82bc-f447047e5290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading with torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jellyho/miniconda3/envs/mobilevlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type mobilevlm to instantiate a model of type spatialvla. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of SpatialVLAForCausalLM were not initialized from the model checkpoint at remyxai/SpaceLLaVA-lite and are newly initialized: ['action_head.action_proj.2.weight', 'action_head.noise_pos', 'action_head.time_net.out_net.0.bias', 'action_head.time_net.w', 'action_head.time_net.out_net.0.weight', 'action_head.eps_net.linear.bias', 'action_head.eps_net.adaLN_modulation.1.weight', 'action_head.action_proj.0.bias', 'action_head.action_proj.2.bias', 'action_head.time_net.out_net.2.bias', 'action_head.timestep_pos', 'action_head.eps_net.adaLN_modulation.1.bias', 'action_head.action_proj.0.weight', 'action_head.eps_net.linear.weight', 'action_head.time_net.out_net.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model, image_processor, _ = load_pretrained_vlm_for_vla(\n",
    "    model_args, \n",
    "    load_8bit=False, \n",
    "    load_4bit=False,\n",
    "    device=device_id,\n",
    "    dtype=dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aff9f47-30f6-429c-b1d9-ddefbd813c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4fd9ff5-1e51-46b9-b1c1-cf506f7f1014",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b333824-6205-40f6-964e-c3551e0d4b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 14:27:18.128313: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-11-24 14:27:18.527325: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################################\n",
      "# Loading the following 1 datasets (incl. sampling weight):                         #\n",
      "# libero_object_no_noops: ==================================================1.000000 #\n",
      "######################################################################################\n",
      "\n",
      "Threads per Dataset: %s [1]\n",
      "Reads per Dataset: %s [1]\n",
      "Constructing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 14:27:18.926053: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying frame transforms on dataset...\n"
     ]
    }
   ],
   "source": [
    "batch_transform = RLDSBatchTransform(\n",
    "    tokenizer,\n",
    "    image_processor,\n",
    ")\n",
    "\n",
    "dataset = RLDSDataset(\n",
    "    data_root_dir=training_args.data_root_dir,\n",
    "    data_mix=training_args.data_mix,\n",
    "    batch_transform=batch_transform,\n",
    "    shuffle_buffer_size=training_args.shuffle_buffer_size,\n",
    "    train=True,\n",
    "    window_size=1,\n",
    "    future_action_window_size=model_args.action_len - 1\n",
    ")\n",
    "collator = PaddedCollatorForActionPrediction(tokenizer.model_max_length, tokenizer.pad_token_id, padding_side='right')\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=training_args.batch_size,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5276f31-55bc-490a-8d25-29c4035d283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx, batch = next(enumerate(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83517486-04e4-4d1d-b3d7-2ff1c5dd0c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values', 'input_ids', 'attention_mask', 'action', 'dataset_names'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "def8775f-fce7-4a84-9d4f-fc4b59536b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=batch['input_ids'].to(device_id)\n",
    "images=batch['pixel_values'].to(device_id)\n",
    "attention_mask=batch['attention_mask'].to(device_id)\n",
    "actions=batch['action'].to(device_id)\n",
    "past_key_values = None\n",
    "labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "455e87b0-f283-436e-9ccc-787f0025bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast('cuda', dtype=dtype):\n",
    "    input_ids, attention_mask, past_key_values, inputs_embeds, labels = model.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\n",
    "    input_ids, attention_mask, past_key_values, inputs_embeds, labels, time_enc, noise =  model.action_head.prepare_inputs_for_DiT(actions, input_ids, attention_mask, past_key_values, inputs_embeds, labels)\n",
    "    outputs = model.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=False,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "    hidden = outputs[0].contiguous()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3bd141b-432c-4a7c-8c58-118193157de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 2048])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efcd6a4a-116a-465e-b827-49f175dfafa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 2048]) torch.Size([32, 1, 2048])\n",
      "torch.Size([32, 1, 2048])\n"
     ]
    }
   ],
   "source": [
    "output_tokens = hidden[:, -model.config.action_len:, :]\n",
    "eps_out = model.action_head.eps_net(output_tokens, time_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "638bdb01-dd5b-41c4-92e9-e84f58b9668f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06fe973a-ec50-48d5-9387-52de68910964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "loss = nn.functional.mse_loss(eps_out, noise, reduction='none')\n",
    "loss = loss.sum(1).mean() # Sum over the actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac5e8167-000d-48ec-8929-023b71f084c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8999, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
